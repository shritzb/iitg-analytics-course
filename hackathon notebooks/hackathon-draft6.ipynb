{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12105345,"sourceType":"datasetVersion","datasetId":7621160}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T16:39:48.546774Z","iopub.execute_input":"2025-06-09T16:39:48.547099Z","iopub.status.idle":"2025-06-09T16:39:51.632394Z","shell.execute_reply.started":"2025-06-09T16:39:48.547069Z","shell.execute_reply":"2025-06-09T16:39:51.631298Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hackathon-set/hacktest.csv\n/kaggle/input/hackathon-set/hacktrain.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMClassifier\nfrom scipy.stats import mode\nfrom lightgbm import early_stopping, log_evaluation\n\n# Load data\ntrain_df = pd.read_csv(\"/kaggle/input/hackathon-set/hacktrain.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/hackathon-set/hacktest.csv\")\n\ntrain_df = train_df.drop(columns=[\"Unnamed: 0\"])\ntest_df = test_df.drop(columns=[\"Unnamed: 0\"])\n\nle = LabelEncoder()\ntrain_df[\"class\"] = le.fit_transform(train_df[\"class\"])\n\ny = train_df[\"class\"].values\nX = train_df.drop(columns=[\"ID\", \"class\"])\ntest_ids = test_df[\"ID\"].values\ntest_X = test_df.drop(columns=[\"ID\"])\n\nX = X.fillna(X.mean())\ntest_X = test_X.fillna(X.mean())\n\ndef add_features(df):\n    df = df.copy()\n    df[\"mean\"] = df.mean(axis=1)\n    df[\"std\"] = df.std(axis=1)\n    df[\"max\"] = df.max(axis=1)\n    df[\"min\"] = df.min(axis=1)\n    df[\"range\"] = df[\"max\"] - df[\"min\"]\n\n    # Polynomial features (squares)\n    for col in [\"mean\", \"std\", \"max\", \"min\", \"range\"]:\n        df[f\"{col}_sq\"] = df[col] ** 2\n\n    # Pairwise products (just a few, not all combos)\n    df[\"mean_std\"] = df[\"mean\"] * df[\"std\"]\n    df[\"max_min\"] = df[\"max\"] * df[\"min\"]\n    df[\"mean_range\"] = df[\"mean\"] * df[\"range\"]\n\n    return df\n\nX = add_features(X)\ntest_X = add_features(test_X)\n\nX.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\ntest_X.columns = test_X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntest_preds = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"Training fold {fold+1}\")\n    model = LGBMClassifier(\n        objective=\"multiclass\",\n        num_class=len(np.unique(y)),\n        learning_rate=0.05,\n        max_depth=8,\n        num_leaves=31,\n        n_estimators=500,\n        min_child_samples=20,   # added parameter\n        random_state=42\n    )\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        callbacks=[\n            early_stopping(stopping_rounds=20),\n            log_evaluation(period=10)\n        ]\n    )\n\n    preds = model.predict(test_X)\n    test_preds.append(preds)\n\nfinal_preds = mode(np.array(test_preds), axis=0).mode.flatten()\nfinal_labels = le.inverse_transform(final_preds)\n\nsubmission = pd.DataFrame({\"ID\": test_ids, \"class\": final_labels})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"âœ… Submission file saved! Ready to submit.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T16:41:39.382760Z","iopub.execute_input":"2025-06-09T16:41:39.383458Z","iopub.status.idle":"2025-06-09T16:42:01.669795Z","shell.execute_reply.started":"2025-06-09T16:41:39.383429Z","shell.execute_reply":"2025-06-09T16:42:01.668685Z"}},"outputs":[{"name":"stdout","text":"Training fold 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10200\n[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 40\n[LightGBM] [Info] Start training from score -2.252308\n[LightGBM] [Info] Start training from score -0.261568\n[LightGBM] [Info] Start training from score -3.707807\n[LightGBM] [Info] Start training from score -2.481787\n[LightGBM] [Info] Start training from score -5.585999\n[LightGBM] [Info] Start training from score -4.333236\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's multi_logloss: 0.404533\n[20]\tvalid_0's multi_logloss: 0.29908\n[30]\tvalid_0's multi_logloss: 0.24524\n[40]\tvalid_0's multi_logloss: 0.216435\n[50]\tvalid_0's multi_logloss: 0.200865\n[60]\tvalid_0's multi_logloss: 0.190521\n[70]\tvalid_0's multi_logloss: 0.186215\n[80]\tvalid_0's multi_logloss: 0.18444\n[90]\tvalid_0's multi_logloss: 0.183509\n[100]\tvalid_0's multi_logloss: 0.182369\n[110]\tvalid_0's multi_logloss: 0.182589\n[120]\tvalid_0's multi_logloss: 0.183693\nEarly stopping, best iteration is:\n[107]\tvalid_0's multi_logloss: 0.18205\nTraining fold 2\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001751 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10200\n[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 40\n[LightGBM] [Info] Start training from score -2.252308\n[LightGBM] [Info] Start training from score -0.261568\n[LightGBM] [Info] Start training from score -3.707807\n[LightGBM] [Info] Start training from score -2.481787\n[LightGBM] [Info] Start training from score -5.585999\n[LightGBM] [Info] Start training from score -4.333236\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's multi_logloss: 0.401876\n[20]\tvalid_0's multi_logloss: 0.291978\n[30]\tvalid_0's multi_logloss: 0.24104\n[40]\tvalid_0's multi_logloss: 0.215075\n[50]\tvalid_0's multi_logloss: 0.198176\n[60]\tvalid_0's multi_logloss: 0.188193\n[70]\tvalid_0's multi_logloss: 0.182228\n[80]\tvalid_0's multi_logloss: 0.179114\n[90]\tvalid_0's multi_logloss: 0.177542\n[100]\tvalid_0's multi_logloss: 0.176801\n[110]\tvalid_0's multi_logloss: 0.176553\n[120]\tvalid_0's multi_logloss: 0.176105\n[130]\tvalid_0's multi_logloss: 0.176701\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[117]\tvalid_0's multi_logloss: 0.175963\nTraining fold 3\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001782 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10200\n[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 40\n[LightGBM] [Info] Start training from score -2.252308\n[LightGBM] [Info] Start training from score -0.261568\n[LightGBM] [Info] Start training from score -3.707807\n[LightGBM] [Info] Start training from score -2.481787\n[LightGBM] [Info] Start training from score -5.585999\n[LightGBM] [Info] Start training from score -4.333236\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's multi_logloss: 0.406982\n[20]\tvalid_0's multi_logloss: 0.301803\n[30]\tvalid_0's multi_logloss: 0.251047\n[40]\tvalid_0's multi_logloss: 0.222302\n[50]\tvalid_0's multi_logloss: 0.20872\n[60]\tvalid_0's multi_logloss: 0.201218\n[70]\tvalid_0's multi_logloss: 0.195652\n[80]\tvalid_0's multi_logloss: 0.194221\n[90]\tvalid_0's multi_logloss: 0.193047\n[100]\tvalid_0's multi_logloss: 0.192754\n[110]\tvalid_0's multi_logloss: 0.193655\nEarly stopping, best iteration is:\n[97]\tvalid_0's multi_logloss: 0.192512\nTraining fold 4\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001765 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10200\n[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 40\n[LightGBM] [Info] Start training from score -2.253795\n[LightGBM] [Info] Start training from score -0.261568\n[LightGBM] [Info] Start training from score -3.707807\n[LightGBM] [Info] Start training from score -2.479919\n[LightGBM] [Info] Start training from score -5.585999\n[LightGBM] [Info] Start training from score -4.333236\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's multi_logloss: 0.408826\n[20]\tvalid_0's multi_logloss: 0.303224\n[30]\tvalid_0's multi_logloss: 0.253674\n[40]\tvalid_0's multi_logloss: 0.226837\n[50]\tvalid_0's multi_logloss: 0.213348\n[60]\tvalid_0's multi_logloss: 0.206782\n[70]\tvalid_0's multi_logloss: 0.203202\n[80]\tvalid_0's multi_logloss: 0.201474\n[90]\tvalid_0's multi_logloss: 0.199749\n[100]\tvalid_0's multi_logloss: 0.199855\nEarly stopping, best iteration is:\n[89]\tvalid_0's multi_logloss: 0.199522\nTraining fold 5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10200\n[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 40\n[LightGBM] [Info] Start training from score -2.252308\n[LightGBM] [Info] Start training from score -0.261365\n[LightGBM] [Info] Start training from score -3.714197\n[LightGBM] [Info] Start training from score -2.481787\n[LightGBM] [Info] Start training from score -5.585999\n[LightGBM] [Info] Start training from score -4.333236\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's multi_logloss: 0.4116\n[20]\tvalid_0's multi_logloss: 0.305316\n[30]\tvalid_0's multi_logloss: 0.255548\n[40]\tvalid_0's multi_logloss: 0.225461\n[50]\tvalid_0's multi_logloss: 0.209766\n[60]\tvalid_0's multi_logloss: 0.20053\n[70]\tvalid_0's multi_logloss: 0.196823\n[80]\tvalid_0's multi_logloss: 0.193102\n[90]\tvalid_0's multi_logloss: 0.19143\n[100]\tvalid_0's multi_logloss: 0.190741\n[110]\tvalid_0's multi_logloss: 0.190827\n[120]\tvalid_0's multi_logloss: 0.192292\nEarly stopping, best iteration is:\n[104]\tvalid_0's multi_logloss: 0.19019\nâœ… Submission file saved! Ready to submit.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}